{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data = loadmat('Data/sarcos_inv.mat')['sarcos_inv']\n",
    "full_test_data = loadmat('Data/sarcos_inv_test.mat')['sarcos_inv_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData (full_train_data, full_test_data, num_train, num_validation, num_test):\n",
    "    \n",
    "    idx_train = np.random.randint(full_train_data.shape[0], size=num_train)\n",
    "    idx_validation = np.random.randint(full_train_data.shape[0], size=num_validation)\n",
    "    idx_test = np.random.randint(full_test_data.shape[0], size=num_test)\n",
    "    \n",
    "    train_data = full_train_data[idx_train, : ]\n",
    "    validation_data = full_train_data[idx_validation, :]\n",
    "    test_data = full_test_data[idx_test, : ]\n",
    "    return (train_data, validation_data, test_data)\n",
    "\n",
    "def Sep_X_and_Y(data, x_dim, y_dim):\n",
    "    X = data[:, :x_dim]\n",
    "    Y = data[:, x_dim: x_dim + y_dim ]\n",
    "    return X, Y\n",
    "\n",
    "def Unfold_Y(X, Y):\n",
    "    num_tasks = Y.shape[1]\n",
    "    X_new = np.vstack([X]*num_tasks)\n",
    "    Y_new = np.ndarray.flatten(Y, 'F')\n",
    "    T = np.arange(num_tasks)\n",
    "    T_new = np.repeat(T, X.shape[0])\n",
    "    return X_new, Y_new, T_new\n",
    "\n",
    "def append_one(X):\n",
    "    n = len(X)\n",
    "    ones = np.zeros((n, 1)) + 1\n",
    "    X_new = np.concatenate([ones, X], axis= 1)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, validation_data, test_data) = getData(full_train_data, full_test_data, 500, 100, 500)\n",
    "\n",
    "X_train_org, Y_train_org = Sep_X_and_Y(train_data, 21, 2)\n",
    "X_valid_org, Y_valid_org = Sep_X_and_Y(validation_data, 21, 2)\n",
    "X_test_org , Y_test_org  = Sep_X_and_Y(test_data, 21, 2)\n",
    "\n",
    "X_train, Y_train, T_train = Unfold_Y(X_train_org, Y_train_org)\n",
    "X_valid, Y_valid, T_valid = Unfold_Y(X_valid_org, Y_valid_org)\n",
    "X_test , Y_test , T_test  = Unfold_Y(X_test_org , Y_test_org )\n",
    "\n",
    "X_train = append_one(X_train)\n",
    "X_valid = append_one(X_valid)\n",
    "X_test = append_one(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kernel_input(x1, x2):  # K_input part of ICM kernel\n",
    "    numerator    = 2*np.dot(np.dot(x1, Sigma_u), x2 )\n",
    "    denominator1 = 1 + 2*np.dot(np.dot(x1, Sigma_u), x1 )\n",
    "    denominator2 = 1 + 2*np.dot(np.dot(x2, Sigma_u), x2 )\n",
    "    denominator  = math.sqrt(denominator1*denominator2) # See \"Computing with infinite networks\" for calculation of Expectation term \n",
    "    \n",
    "    Expectation_term = (2/np.pi) * math.asin( numerator/denominator )\n",
    "    similarity       = C_term +  Expectation_term  # See \"Multitask Neural networks meet Multitask Gaussian Process\" Paper for notation of C-term and Expectation term\n",
    "    return similarity \n",
    "\n",
    "\n",
    "def Kernel_task (t1, t2):   # K_task part of the ICM Kernels\n",
    "    return Omega2[t1, t2]\n",
    "\n",
    "\n",
    "def Kernel(x1, t1, x2, t2): # ICM Kernel - product of input and task dependent components\n",
    "    return Kernel_input(x1, x2)*Kernel_task(t1, t2)\n",
    "\n",
    "\n",
    "def mtgp_fit (X_train, T_train, Y_train, Noise_variance ): # Simple MTGP implementation with specified Kernels (here ICM kernels)\n",
    "    N = len(X_train)\n",
    "    K = np.zeros((N, N))\n",
    "        \n",
    "    for i in range(0, N):\n",
    "        for j in range(0, N):\n",
    "            K[i, j] = Kernel(X_train[i], T_train[i], X_train[j], T_train[j] )\n",
    "            \n",
    "    B = np.zeros((N, N))\n",
    "    for i in range(0, N):\n",
    "        B[i, i] = Noise_variance[T_train[i]]\n",
    "         \n",
    "    \n",
    "    C = K + B\n",
    "    C_inv = np.linalg.inv(C)\n",
    "    alpha = np.dot(C_inv, Y_train)\n",
    "    model = {\n",
    "        \"X_train\" :  X_train,\n",
    "        \"T_train\" :  T_train,\n",
    "        \"Y_train\" :  Y_train,\n",
    "        \"Noise_variance\" : Noise_variance,\n",
    "        \"C_inv\"   :  C_inv,\n",
    "        \"alpha\"   :  alpha,\n",
    "    }\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def mtgp_predict(X, T, model):\n",
    "    \n",
    "    n_train  = len(model[\"X_train\"])\n",
    "    n        = len(X)\n",
    "    K        = np.zeros((n_train, n))\n",
    "    \n",
    "    for i in range(0, n_train):\n",
    "        for j in range(0, n):\n",
    "            K[i, j] = Kernel( model[\"X_train\"][i], model[\"T_train\"][i], X[j] , T[j] )\n",
    "            \n",
    "    y_pred = np.dot(K.T, model[\"alpha\"] )\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def get_stats (y_true, y_predict):\n",
    "    msr = mean_squared_error(y_true, y_predict)\n",
    "    stats ={\n",
    "        \"msr\" : msr\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary variables for convineance - remove after tuning\n",
    "\n",
    "temp_21_vec = np.zeros(22) + 1\n",
    "temp_2x2    = np.identity(2)\n",
    "temp_2      = [1e-5, 1e-5]\n",
    "\n",
    "# Kernel Hyperparameters - Prior Hyperparameters\n",
    "\n",
    "Sigma_u =  np.diag( temp_21_vec ) # diagonal matrix of size = dimensions of X (current example: 21)\n",
    "C_term  =  1             # Proportionality Constant term for bias variance \n",
    "Omega2  =  temp_2x2      # Covariance between tasks - Symmetric matrix of size T X T \n",
    "\n",
    "\n",
    "# Noise Variance - Likelihood Hyperparameters\n",
    "Noise_variance = temp_2 # Noise variance - Vector of size T, number of tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Negative_Loss (a):\n",
    "    global Noise_variance\n",
    "    global Sigma_u\n",
    "    Sigma_u = math.pow(10, a)*np.diag( temp_21_vec )\n",
    "#     Noise_variance = np.array([math.pow(10, Noise_variance1), math.pow(10, Noise_variance2) ])\n",
    "    model = mtgp_fit (X_train, T_train, Y_train, Noise_variance )\n",
    "    predict = mtgp_predict(X_valid, T_valid, model)\n",
    "    stats = get_stats(Y_valid, predict)\n",
    "    return -stats['msr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "            \"a\" : (-10, 2)\n",
    "          }\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f= get_Negative_Loss,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-24.75   \u001b[0m | \u001b[0m-4.996   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-80.09   \u001b[0m | \u001b[0m-1.356   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-24.75   \u001b[0m | \u001b[95m-4.996   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-24.59   \u001b[0m | \u001b[95m-4.961   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-24.45   \u001b[0m | \u001b[95m-4.933   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-23.68   \u001b[0m | \u001b[95m-4.787   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-22.76   \u001b[0m | \u001b[95m-4.62    \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-21.58   \u001b[0m | \u001b[95m-4.427   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-20.52   \u001b[0m | \u001b[95m-4.243   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-22.99   \u001b[0m | \u001b[0m-4.068   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-20.62   \u001b[0m | \u001b[0m-4.286   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-20.53   \u001b[0m | \u001b[0m-4.253   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23.77582466812858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "num_hidd_units = 1000\n",
    "\n",
    "for rs in range(0, 1):\n",
    "    clf = MLPRegressor(hidden_layer_sizes= (num_hidd_units, ), activation='logistic' ,random_state=rs,  solver='adam', max_iter=100000)\n",
    "    clf.fit(X_train_org, Y_train_org)\n",
    "    Y_pred_NN = clf.predict(X_valid_org)\n",
    "    Y_pred_NN1 = Y_pred_NN.flatten()\n",
    "    Y_valid_org1 = Y_valid_org.flatten()\n",
    "    \n",
    "    stats = get_stats(Y_pred_NN1, Y_valid_org1)\n",
    "    print( -stats['msr'] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
